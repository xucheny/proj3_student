\documentclass[]{article}
\begin{document}

0. Softmax Regression

Files to edit/turn in:\\
softmax.py\\
report.pdf\\
partners.txt

The goal of this part of the project is to implement Softmax Regression in 
order to classify the MNIST digit dataset. Softmax Regression is essentially a 
two-layer neural network where the output layer applies the Softmax cost 
function, a multiclass generalization of the logistic cost function. 

In logistic regression, we have a hypothesis function of the form

$P[y = 1] = \frac{1}{1+e^{-\vec{w}\cdot\vec{x}}}$

where $\vec{w}$ is our weight vector. Like the hyperbolic tangent function, the 
logistic function is also a sigmoid function with the characteristic 's'-like 
shape, though it has a range of (0, 1) instead of (-1, 1). Note that this is 
technically not a classifier since it returns probabilities instead of a predicted 
class, but it's easy to turn it into a classifier by simply choosing the class with 
the highest probability.

Since logistic regression is used for binary classification, it is easy to see 
that
$$P[y = 1] = \frac{1}{1+e^{-\vec{w}\cdot\vec{x}}} \\
      = \frac{e^{\vec{w}\cdot\vec{x}}}{e^{\vec{w}\cdot\vec{x}}+1} \\
      = 
      \frac{e^{\vec{w}\cdot\vec{x}}}{e^{\vec{w}\cdot\vec{x}}+e^{\vec{0}\cdot\vec{x}}}$$

Similarly, 
$$P[y = 1] = 1 - \frac{1}{1+e^{-\vec{w}\cdot\vec{x}}} \\
         = \frac{e^{\vec{w}\cdot\vec{x}}+1}{e^{\vec{w}\cdot\vec{x}}+1} - 
         \frac{e^{\vec{w}\cdot\vec{x}}}{e^{\vec{w}\cdot\vec{x}}+1} \\
         = 
\frac{e^{\vec{0}\cdot\vec{x}}}{e^{\vec{w}\cdot\vec{x}}+e^{\vec{0}\cdot\vec{x}}}$$

From this form it appears that we can assign the vector $\vec{w_1} = \vec{w}$ 
as the weight vector for class 1 and $\vec{w_0} = \vec{0}$ as the weight vector 
for class 0. Our probability formulas are now unified into one equation:

$$P[y = i] = \frac{e^{\vec{w_i}\cdot\vec{x}}}{\sum_{j}e^{\vec{w_j}\cdot\vec{x}}}$$

This immediately motivates generalization to classification with more than 2 classes. 
By assigning a separate weight vector $\vec{w_i}$ to each class, for each example 
$\vec{x}$ we can predict the probability that it is class $i$, and again we can
classify by choosing the most probable class. A more compact way of representing the
values $\vec{w_i}\cdot\vec{x}$ is $W \vec{x}$ where each row $i$ of W is $\vec{w_i}$.
We can also represent a dataset $\{\vec{x_i}\}$ with a matrix $X$ where each column is
a single example.

Q0:\\
    (1) Show that the probabilities sum to 1. \\
    (2) What are the dimensions of $W$? $X$? $WX$?

We can also train on this model with an appropriate loss function. The Softmax loss function is given by

$$L(W) = -\left[ \sum_{i=1}^{m} \sum_{k=1}^{K} 1\left\{y_i = k\right\} \log \frac{e^{\vec{w_k}\cdot \vec{x_i}}}{\sum_{j=1}^K e^{\vec{w_j}\cdot \vec{x_i}}}\right]$$

where $m$ is the number of examples, $k$ is the number of classes, and $1\left\{y_i = k\right\}$
is an indicator variable that equals 1 when the statement inside the brackets is true, and 0
otherwise. The gradient (which you will not derive) is given by

$$\nabla_{\vec{w_k}} L(W) = -\sum_{i=1}^{m}{ \left[ \vec{x_i} \left( 1\{y_i = k\}  - P[y_i = k]\right) \right]}$$

Note that the indicator and the probabilities can be represented as matrices, which makes the code
for the loss and the gradient very simple.

(See http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/ for more 
details)

softmax.py contains a mostly-complete implementation of Softmax Regression. 
A code stub also has been provided in run\_softmax.py. Once you correctly implement
the incomplete portions of softmax.py, you will be able to run run\_softmax.py
in order to classify the MNIST digits.

Q1:\\
    (1) Complete the implementation of the cost function.\\
    (2) Complete the implementation of the predict function.

Q2: When initializing the weight matrix, in some cases it may be appropriate to 
initialize the entries as small random numbers rather than all zeros. (This is implemented in a commented line in the \_\_init\_\_ function; experiment with the effect of the initialization if you like.) Give one 
reason why this may be a good idea (with justification).

Q3: In the cost function, we see the line\\
W\_X = W\_X - np.max(W\_X)

This means that each entry is reduced by the largest entry in the matrix.
    (1) Show that this does not affect the predicted probabilities.
    (2) Why might this be an optimization over using W\_X? Justify your answer.

Q4: Use the learningCurve function in runClassifier.py to plot the accuracy of 
the classifier as a function of the number of examples seen. Include the plot in your writeup. Do you observe any
overfitting or underfitting? Justify.

Q5: Use the hyperparamCurve function to plot the accuracy of the classifier as 
a function of the number of iterations. (Note that the default is 400 iterations.) Include the plot in your writeup. About
what value of maxIter gives you the best accuracy?

\end{document}
